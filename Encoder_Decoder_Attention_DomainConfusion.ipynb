{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Encoder-Decoder-Attention-DomainConfusion",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "fkPfnEj9dnKd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git clone https://github.com/soumyac1999/Music-Translation.git"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LM_Ct8kkd3Hp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mv Music-Translation/Data Data/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iB0s_KWdcGJj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pip install --upgrade soundfile librosa"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g9zRa7WneyCc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import librosa\n",
        "import soundfile as sf\n",
        "import shutil, os\n",
        "# import argparse\n",
        "\n",
        "# parser = argparse.ArgumentParser()\n",
        "\n",
        "# parser.add_argument('--src', help=\"Src root\", required=True)\n",
        "# parser.add_argument('--dst', help=\"Dest root\", required=True)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\t# args = parser.parse_args()\n",
        "\tsrc_root = 'Data/'\n",
        "\tdest_root = 'Preprocessed/'\n",
        "\tfor instrument_name in os.listdir(src_root): # instrument wise\n",
        "\t\tprint(instrument_name)\n",
        "\t\tfor data_split in os.listdir(os.path.join(src_root, instrument_name)): # split dir\n",
        "\t\t\tfor filename in os.listdir(os.path.join(src_root, instrument_name, data_split)):\n",
        "\t\t\t\t# do  something about it -> process\n",
        "\t\t\t\t# now load and save npy files\n",
        "\t\t\t\tsrcpath = os.path.join(src_root, instrument_name, data_split, filename)\n",
        "\t\t\t\ty, sr = librosa.load(srcpath, sr=32000)\n",
        "\t\t\t\tS = librosa.feature.mfcc(y, sr=sr)\n",
        "\t\t\t\tdstpath = os.path.join(dest_root, instrument_name, data_split, filename.split('.')[0])\n",
        "\t\t\t\tos.makedirs(os.path.join(dest_root, instrument_name, data_split), exist_ok=True)\n",
        "\t\t\t\tnp.save(dstpath, S)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YiiBQa_LslXp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# https://github.com/bentrevett/pytorch-seq2seq/blob/master/1%20-%20Sequence%20to%20Sequence%20Learning%20with%20Neural%20Networks.ipynb\n",
        "import IPython.display as ipd\n",
        "import librosa\n",
        "import librosa.display\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import soundfile as sf\n",
        "import random\n",
        "\n",
        "# y, sr = librosa.load('2342.wav', duration=5, sr=16000)\n",
        "# # sf.write('2330_0.wav', y, sr)\n",
        "# # ipd.Audio('2330_0.wav')\n",
        "# D = np.abs(librosa.stft(y))**2\n",
        "# S = librosa.feature.melspectrogram(S=D, sr=sr, n_mels=64, fmax=8000)\n",
        "# plt.subplot(121)\n",
        "# S_dB = librosa.power_to_db(S, ref=np.max)\n",
        "# librosa.display.specshow(S_dB, x_axis='time', y_axis='mel', sr=sr, fmax=8000)\n",
        "# plt.colorbar(format='%+2.0f dB')\n",
        "# plt.title('Mel-frequency spectrogram')\n",
        "# plt.tight_layout()\n",
        "# # plt.show()\n",
        "\n",
        "# y_hat = librosa.feature.inverse.mel_to_audio(S, sr)\n",
        "# # sf.write('2330_1.wav', y_hat, sr)\n",
        "# # ipd.Audio('2330_1.wav')\n",
        "# ipd.Audio(y_hat, rate=sr)\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "device = 'cuda:0'\n",
        "class InstrumentDataset(object):\n",
        "\tdef __init__(self, datadir, normalize=False):\n",
        "\t\tsuper(InstrumentDataset, self).__init__()\n",
        "\t\tself.datadir = datadir\n",
        "\t\tself.files = os.listdir(datadir)\n",
        "\t\tself.len = len(self.files)\n",
        "\t\tself.normalize = normalize\n",
        "\t\tself.ylen = 157\n",
        "\n",
        "\tdef __len__(self):\n",
        "\t\treturn self.len\n",
        "\n",
        "\tdef __getitem__(self, idx):\n",
        "\t\tS = np.load(os.path.join(self.datadir, self.files[idx]))\n",
        "\t\t# y, sr = librosa.load(os.path.join(self.datadir, self.files[idx]), duration=5, sr=16000)\n",
        "\t\t# D = np.abs(librosa.stft(y))**2\n",
        "\t\t# S = librosa.feature.melspectrogram(S=D, sr=sr, n_mels=64, fmax=8000)\n",
        "\t\tx = torch.tensor(S.T, dtype=torch.float)\n",
        "\t\treturn x\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, enc_hid_dim, dec_hid_dim, n_layers, dropout):\n",
        "        super(Encoder,self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.enc_hid_dim = enc_hid_dim\n",
        "        self.dec_hid_dim = dec_hid_dim\n",
        "        self.n_layers = n_layers\n",
        "        self.rnn = nn.GRU(input_dim, enc_hid_dim, n_layers, bidirectional = True)\n",
        "        \n",
        "        self.fc = nn.Linear(enc_hid_dim * 2, dec_hid_dim)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        # self.dropout = lambda x: x\n",
        "        \n",
        "    def forward(self, src):\n",
        "        \n",
        "        #src = [src sent len, batch size]\n",
        "        \n",
        "        embedded = src\n",
        "        \n",
        "        #embedded = [src sent len, batch size, emb dim]\n",
        "        \n",
        "        outputs, hidden = self.rnn(embedded)\n",
        "                \n",
        "        #outputs = [src sent len, batch size, hid dim * num directions]\n",
        "        #hidden = [n layers * num directions, batch size, hid dim]\n",
        "        \n",
        "        #hidden is stacked [forward_1, backward_1, forward_2, backward_2, ...]\n",
        "        #outputs are always from the last layer\n",
        "        \n",
        "        #hidden [-2, :, : ] is the last of the forwards RNN \n",
        "        #hidden [-1, :, : ] is the last of the backwards RNN\n",
        "        \n",
        "        #initial decoder hidden is final hidden state of the forwards and backwards \n",
        "        #  encoder RNNs fed through a linear layer\n",
        "        hidden = torch.tanh(self.fc(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)))\n",
        "\n",
        "        #outputs = [src sent len, batch size, enc hid dim * 2]\n",
        "        #hidden = [batch size, dec hid dim]\n",
        "        # print(\"Values returned by encoder: \", outputs.shape, hidden.shape)\n",
        "        return outputs, hidden\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, enc_hid_dim, dec_hid_dim):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.attn = nn.Linear((enc_hid_dim * 2) + dec_hid_dim, dec_hid_dim)\n",
        "        self.v = nn.Parameter(torch.rand(dec_hid_dim))\n",
        "        \n",
        "    def forward(self, hidden, encoder_outputs):\n",
        "        \n",
        "        #hidden = [batch size, dec hid dim]\n",
        "        #encoder_outputs = [src sent len, batch size, enc hid dim * 2]\n",
        "        \n",
        "        batch_size = encoder_outputs.shape[1]\n",
        "        src_len = encoder_outputs.shape[0]\n",
        "        \n",
        "        #repeat encoder hidden state src_len times\n",
        "        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)\n",
        "        \n",
        "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
        "        \n",
        "        #hidden = [batch size, src sent len, dec hid dim]\n",
        "        #encoder_outputs = [batch size, src sent len, enc hid dim * 2]\n",
        "        \n",
        "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim = 2))) \n",
        "        \n",
        "        #energy = [batch size, src sent len, dec hid dim]\n",
        "        \n",
        "        energy = energy.permute(0, 2, 1)\n",
        "        \n",
        "        #energy = [batch size, dec hid dim, src sent len]\n",
        "        \n",
        "        #v = [dec hid dim]\n",
        "        \n",
        "        v = self.v.repeat(batch_size, 1).unsqueeze(1)\n",
        "\n",
        "        #v = [batch size, 1, dec hid dim]\n",
        "                \n",
        "        attention = torch.bmm(v, energy).squeeze(1)\n",
        "        \n",
        "        #attention= [batch size, src len]\n",
        "        \n",
        "        return torch.nn.functional.softmax(attention, dim=1)\n",
        "        \n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_dim, enc_hid_dim, dec_hid_dim, n_layers, dropout, attention):\n",
        "        super().__init__()\n",
        "\n",
        "        self.output_dim = output_dim\n",
        "        self.attention = attention\n",
        "                \n",
        "        self.rnn = nn.GRU((enc_hid_dim * 2) + output_dim, dec_hid_dim, n_layers)\n",
        "        \n",
        "        self.out = nn.Linear((enc_hid_dim * 2) + dec_hid_dim + output_dim, output_dim)\n",
        "        \n",
        "        # self.dropout = nn.Dropout(dropout)\n",
        "        self.dropout = lambda x: x\n",
        "        \n",
        "    def forward(self, input, hidden, encoder_outputs):\n",
        "             \n",
        "        #input = [batch size]\n",
        "        #hidden = [batch size, dec hid dim]\n",
        "        #encoder_outputs = [src sent len, batch size, enc hid dim * 2]\n",
        "        \n",
        "        input = input.unsqueeze(0)\n",
        "        # print(\"input.size in Decoder forward\", input.size())\n",
        "        #input = [1, batch size]\n",
        "        \n",
        "        embedded = input\n",
        "        \n",
        "        #embedded = [1, batch size, emb dim]\n",
        "        \n",
        "        a = self.attention(hidden, encoder_outputs)\n",
        "                \n",
        "        #a = [batch size, src len]\n",
        "        \n",
        "        a = a.unsqueeze(1)\n",
        "\n",
        "         #a = [batch size, 1, src len]\n",
        "        \n",
        "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
        "        \n",
        "        #encoder_outputs = [batch size, src sent len, enc hid dim * 2]\n",
        "        \n",
        "        weighted = torch.bmm(a, encoder_outputs)\n",
        "        \n",
        "        #weighted = [batch size, 1, enc hid dim * 2]\n",
        "        \n",
        "        weighted = weighted.permute(1, 0, 2)\n",
        "        \n",
        "        #weighted = [1, batch size, enc hid dim * 2]\n",
        "        \n",
        "        rnn_input = torch.cat((embedded, weighted), dim = 2)\n",
        "        \n",
        "        #rnn_input = [1, batch size, (enc hid dim * 2) + emb dim]\n",
        "            \n",
        "        output, hidden = self.rnn(rnn_input, hidden.unsqueeze(0))\n",
        "        \n",
        "        #output = [sent len, batch size, dec hid dim * n directions]\n",
        "        #hidden = [n layers * n directions, batch size, dec hid dim]\n",
        "        \n",
        "        #sent len, n layers and n directions will always be 1 in this decoder, therefore:\n",
        "        #output = [1, batch size, dec hid dim]\n",
        "        #hidden = [1, batch size, dec hid dim]\n",
        "        #this also means that output == hidden\n",
        "        assert (output == hidden).all()\n",
        "        \n",
        "        embedded = embedded.squeeze(0)\n",
        "        output = output.squeeze(0)\n",
        "        weighted = weighted.squeeze(0)\n",
        "        \n",
        "        output = self.out(torch.cat((output, weighted, embedded), dim = 1))\n",
        "        \n",
        "        #output = [bsz, output dim]\n",
        "        # print(\"Output.size() in Decoder forward: \", output.size())\n",
        "        return output, hidden.squeeze(0)\n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder, n_classes, device):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.DCNet = nn.Sequential(nn.Linear(encoder.enc_hid_dim*2,n_classes))\n",
        "        self.device = device\n",
        "        \n",
        "    def forward(self, src, trg, teacher_forcing_ratio = 0.5):\n",
        "        \n",
        "        #src = [src sent len, batch size]\n",
        "        #trg = [trg sent len, batch size]\n",
        "        #teacher_forcing_ratio is probability to use teacher forcing\n",
        "        #e.g. if teacher_forcing_ratio is 0.75 we use teacher forcing 75% of the time\n",
        "        \n",
        "        batch_size = src.shape[1]\n",
        "        max_len = trg.shape[0]\n",
        "        # print(\"max_len: \", max_len)\n",
        "        trg_vocab_size = self.decoder.output_dim\n",
        "        \n",
        "        #tensor to store decoder outputs\n",
        "        outputs = torch.zeros(max_len, batch_size, trg_vocab_size).to(self.device)\n",
        "        \n",
        "        #encoder_outputs is all hidden states of the input sequence, back and forwards\n",
        "        #hidden is the final forward and backward hidden states, passed through a linear layer\n",
        "        encoder_outputs, hidden = self.encoder(src)\n",
        "        predictions = self.DCNet(encoder_outputs[-1,:,:])\n",
        "                \n",
        "        #first input to the decoder is the <sos> tokens\n",
        "        input = trg[0,:]\n",
        "        \n",
        "        # print(\"Input.size in Seq2Seq forward: \", input.size())\n",
        "        for t in range(1, max_len):\n",
        "            #insert input token embedding, previous hidden state and all encoder hidden states\n",
        "            #receive output tensor (predictions) and new hidden state\n",
        "            output, hidden = self.decoder(input, hidden, encoder_outputs)\n",
        "            \n",
        "            #place predictions in a tensor holding predictions for each token\n",
        "            outputs[t] = output\n",
        "            \n",
        "            #decide if we are going to use teacher forcing or not\n",
        "            teacher_force = random.random() < teacher_forcing_ratio\n",
        "            # print(\"Teacher forcing on? \", teacher_force)\n",
        "            #get the highest predicted token from our predictions\n",
        "            # top1 = output.argmax(1) \n",
        "            \n",
        "            #if teacher forcing, use actual next token as next input\n",
        "            #if not, use predicted token\n",
        "            input = trg[t] if teacher_force else output\n",
        "            # print(\"Input shape used for next pass to decoder: \", input.shape)\n",
        "\n",
        "        return outputs,predictions\n",
        "\n",
        "INPUT_DIM = 20\n",
        "OUTPUT_DIM = 20\n",
        "ENC_HID_DIM = 32\n",
        "DEC_HID_DIM = 32\n",
        "ENC_N_LAYERS = 4\n",
        "DEC_N_LAYERS = 1\n",
        "ENC_DROPOUT = 0.2\n",
        "DEC_DROPOUT = 0.2\n",
        "N_CLASSES = 2\n",
        "datasetPaths = ['Preprocessed/Piano/train/','Preprocessed/Tabla/train/']\n",
        "assert N_CLASSES == len(datasetPaths)\n",
        "\n",
        "attn = Attention(ENC_HID_DIM, DEC_HID_DIM)\n",
        "enc = Encoder(INPUT_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_N_LAYERS, ENC_DROPOUT)\n",
        "decs = [Decoder(OUTPUT_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_N_LAYERS, DEC_DROPOUT, attn) for f in range(N_CLASSES)]\n",
        "\n",
        "models = [Seq2Seq(enc, dec, N_CLASSES, device).to(device) for dec in decs]\n",
        "\n",
        "def init_weights(m):\n",
        "    for name, param in m.named_parameters():\n",
        "        if 'weight' in name:\n",
        "            nn.init.normal_(param.data, mean=0, std=0.01)\n",
        "        else:\n",
        "            nn.init.constant_(param.data, 0)\n",
        "optimizers = [optim.Adam(model.parameters()) for model in models]\n",
        "pretrained = False\n",
        "pretrained_path = ''\n",
        "if(pretrained):\n",
        "  for i,model in enumerate(models):\n",
        "    checkpoint = torch.load(pretrained_path + '/content/drive/My Drive/encde/model%d.pth'%(i))\n",
        "    print(checkpoint.keys())\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "else:\n",
        "  for model in models:            \n",
        "    model.apply(init_weights)\n",
        "\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'Each model has {count_parameters(model):,} trainable parameters')\n",
        "\n",
        "criterion1 = nn.MSELoss()\n",
        "criterion2 = nn.CrossEntropyLoss()\n",
        "\n",
        "trainsets = [InstrumentDataset(f, True) for f in datasetPaths]\n",
        "trainloaders = [DataLoader(x, batch_size=75, shuffle=True, num_workers=8) for x in trainsets]\n",
        "\n",
        "for model in models:\n",
        "  model.train()\n",
        "for epoch in range(60):\n",
        "  for f in range(N_CLASSES):\n",
        "    for i,x in enumerate(trainloaders[f]):\n",
        "      x = x.to(device).transpose(0,1)\n",
        "      src = x\n",
        "      tgt = x\n",
        "      predictions_gt = torch.tensor(np.ones(x.shape[1])*f,dtype=torch.long).to(device)\n",
        "      optimizers[f].zero_grad()\n",
        "      output,predictions = models[f](src, tgt)\n",
        "      # predictions is of size (batch_size, n_classes)   \n",
        "      # print(\"Should be same\", output.shape, tgt.shape)\n",
        "\n",
        "      loss1 = criterion1(output, tgt)\n",
        "      loss2 = criterion2(predictions,predictions_gt)\n",
        "      (loss1-loss2).backward(retain_graph=True)\n",
        "      loss2.backward()\n",
        "      # torch.nn.utils.clip_grad_norm_(model.parameters(), 100)\n",
        "      optimizers[f].step()\n",
        "      # print(torch.abs(x[1]-output[1]))\n",
        "      print('epoch: %d'%(epoch), 'iter: %d'%(i), 'class: %d'%(f), 'loss1: %f'%(loss1.item()),'loss2: %f'%(loss2.item()))\n",
        "  if(epoch%5 == 0):\n",
        "    for i,model in enumerate(models):\n",
        "      torch.save({\n",
        "                  'epoch': epoch,\n",
        "                  'model_state_dict':model.state_dict(),\n",
        "                  'optimizer_state_dict':optimizers[i].state_dict()\n",
        "                  },'/content/drive/My Drive/encde/model%d.pth'%(i))\n",
        "\n",
        "# model.eval()\n",
        "# with torch.no_grad():\n",
        "#     output = model(x, x, 0) #turn off teacher forcing\n",
        "#     loss = criterion(x, x)\n",
        "\n",
        "# plt.subplot(122)\n",
        "# output = output.detach().cpu()*sig + mu\n",
        "# S_dB = librosa.power_to_db(output[:,0,:].T, ref=np.max)\n",
        "# librosa.display.specshow(S_dB, x_axis='time', y_axis='mel', sr=sr, fmax=8000)\n",
        "# plt.colorbar(format='%+2.0f dB')\n",
        "# plt.title('Mel-frequency spectrogram')\n",
        "# plt.tight_layout()\n",
        "# plt.show()\n",
        "\n",
        "# y_hat = librosa.feature.inverse.mel_to_audio(output.numpy()[:,0,:].T, sr)\n",
        "# sf.write('2330_2.wav', y_hat, sr)\n",
        "# # ipd.Audio('2330_2.wav')\n",
        "# ipd.Audio(y_hat, rate=sr)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qoTo2toF9nyP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# https://github.com/bentrevett/pytorch-seq2seq/blob/master/1%20-%20Sequence%20to%20Sequence%20Learning%20with%20Neural%20Networks.ipynb\n",
        "import IPython.display as ipd\n",
        "import librosa\n",
        "import librosa.display\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import soundfile as sf\n",
        "import random\n",
        "\n",
        "# y, sr = librosa.load('2342.wav', duration=5, sr=16000)\n",
        "# # sf.write('2330_0.wav', y, sr)\n",
        "# # ipd.Audio('2330_0.wav')\n",
        "# D = np.abs(librosa.stft(y))**2\n",
        "# S = librosa.feature.melspectrogram(S=D, sr=sr, n_mels=64, fmax=8000)\n",
        "# plt.subplot(121)\n",
        "# S_dB = librosa.power_to_db(S, ref=np.max)\n",
        "# librosa.display.specshow(S_dB, x_axis='time', y_axis='mel', sr=sr, fmax=8000)\n",
        "# plt.colorbar(format='%+2.0f dB')\n",
        "# plt.title('Mel-frequency spectrogram')\n",
        "# plt.tight_layout()\n",
        "# # plt.show()\n",
        "\n",
        "# y_hat = librosa.feature.inverse.mel_to_audio(S, sr)\n",
        "# # sf.write('2330_1.wav', y_hat, sr)\n",
        "# # ipd.Audio('2330_1.wav')\n",
        "# ipd.Audio(y_hat, rate=sr)\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "device = 'cuda:0'\n",
        "class InstrumentDataset(object):\n",
        "\tdef __init__(self, datadir, normalize=False):\n",
        "\t\tsuper(InstrumentDataset, self).__init__()\n",
        "\t\tself.datadir = datadir\n",
        "\t\tself.files = os.listdir(datadir)\n",
        "\t\tself.len = len(self.files)\n",
        "\t\tself.normalize = normalize\n",
        "\t\tself.ylen = 157\n",
        "\n",
        "\tdef __len__(self):\n",
        "\t\treturn self.len\n",
        "\n",
        "\tdef __getitem__(self, idx):\n",
        "\t\tS = np.load(os.path.join(self.datadir, self.files[idx]))\n",
        "\t\t# y, sr = librosa.load(os.path.join(self.datadir, self.files[idx]), duration=5, sr=16000)\n",
        "\t\t# D = np.abs(librosa.stft(y))**2\n",
        "\t\t# S = librosa.feature.melspectrogram(S=D, sr=sr, n_mels=64, fmax=8000)\n",
        "\t\tx = torch.tensor(S.T, dtype=torch.float)\n",
        "\t\tif self.normalize:\n",
        "\t\t\tmu = x.mean(axis=0, keepdims=True)\n",
        "\t\t\tsig = x.std(axis=0, keepdims=True)\n",
        "\t\t\tx = (x-mu)/sig\n",
        "\t\treturn x\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, enc_hid_dim, dec_hid_dim, n_layers, dropout):\n",
        "        super(Encoder,self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.enc_hid_dim = enc_hid_dim\n",
        "        self.dec_hid_dim = dec_hid_dim\n",
        "        self.n_layers = n_layers\n",
        "        self.rnn = nn.GRU(input_dim, enc_hid_dim, n_layers, bidirectional = True)\n",
        "        \n",
        "        self.fc = nn.Linear(enc_hid_dim * 2, dec_hid_dim)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        # self.dropout = lambda x: x\n",
        "        \n",
        "    def forward(self, src):\n",
        "        \n",
        "        #src = [src sent len, batch size]\n",
        "        \n",
        "        embedded = src\n",
        "        \n",
        "        #embedded = [src sent len, batch size, emb dim]\n",
        "        \n",
        "        outputs, hidden = self.rnn(embedded)\n",
        "                \n",
        "        #outputs = [src sent len, batch size, hid dim * num directions]\n",
        "        #hidden = [n layers * num directions, batch size, hid dim]\n",
        "        \n",
        "        #hidden is stacked [forward_1, backward_1, forward_2, backward_2, ...]\n",
        "        #outputs are always from the last layer\n",
        "        \n",
        "        #hidden [-2, :, : ] is the last of the forwards RNN \n",
        "        #hidden [-1, :, : ] is the last of the backwards RNN\n",
        "        \n",
        "        #initial decoder hidden is final hidden state of the forwards and backwards \n",
        "        #  encoder RNNs fed through a linear layer\n",
        "        hidden = torch.tanh(self.fc(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)))\n",
        "\n",
        "        #outputs = [src sent len, batch size, enc hid dim * 2]\n",
        "        #hidden = [batch size, dec hid dim]\n",
        "        # print(\"Values returned by encoder: \", outputs.shape, hidden.shape)\n",
        "        return outputs, hidden\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, enc_hid_dim, dec_hid_dim):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.attn = nn.Linear((enc_hid_dim * 2) + dec_hid_dim, dec_hid_dim)\n",
        "        self.v = nn.Parameter(torch.rand(dec_hid_dim))\n",
        "        \n",
        "    def forward(self, hidden, encoder_outputs):\n",
        "        \n",
        "        #hidden = [batch size, dec hid dim]\n",
        "        #encoder_outputs = [src sent len, batch size, enc hid dim * 2]\n",
        "        \n",
        "        batch_size = encoder_outputs.shape[1]\n",
        "        src_len = encoder_outputs.shape[0]\n",
        "        \n",
        "        #repeat encoder hidden state src_len times\n",
        "        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)\n",
        "        \n",
        "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
        "        \n",
        "        #hidden = [batch size, src sent len, dec hid dim]\n",
        "        #encoder_outputs = [batch size, src sent len, enc hid dim * 2]\n",
        "        \n",
        "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim = 2))) \n",
        "        \n",
        "        #energy = [batch size, src sent len, dec hid dim]\n",
        "        \n",
        "        energy = energy.permute(0, 2, 1)\n",
        "        \n",
        "        #energy = [batch size, dec hid dim, src sent len]\n",
        "        \n",
        "        #v = [dec hid dim]\n",
        "        \n",
        "        v = self.v.repeat(batch_size, 1).unsqueeze(1)\n",
        "\n",
        "        #v = [batch size, 1, dec hid dim]\n",
        "                \n",
        "        attention = torch.bmm(v, energy).squeeze(1)\n",
        "        \n",
        "        #attention= [batch size, src len]\n",
        "        \n",
        "        return torch.nn.functional.softmax(attention, dim=1)\n",
        "        \n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_dim, enc_hid_dim, dec_hid_dim, n_layers, dropout, attention):\n",
        "        super().__init__()\n",
        "\n",
        "        self.output_dim = output_dim\n",
        "        self.attention = attention\n",
        "                \n",
        "        self.rnn = nn.GRU((enc_hid_dim * 2) + output_dim, dec_hid_dim, n_layers)\n",
        "        \n",
        "        self.out = nn.Linear((enc_hid_dim * 2) + dec_hid_dim + output_dim, output_dim)\n",
        "        \n",
        "        # self.dropout = nn.Dropout(dropout)\n",
        "        self.dropout = lambda x: x\n",
        "        \n",
        "    def forward(self, input, hidden, encoder_outputs):\n",
        "             \n",
        "        #input = [batch size]\n",
        "        #hidden = [batch size, dec hid dim]\n",
        "        #encoder_outputs = [src sent len, batch size, enc hid dim * 2]\n",
        "        \n",
        "        input = input.unsqueeze(0)\n",
        "        # print(\"input.size in Decoder forward\", input.size())\n",
        "        #input = [1, batch size]\n",
        "        \n",
        "        embedded = input\n",
        "        \n",
        "        #embedded = [1, batch size, emb dim]\n",
        "        \n",
        "        a = self.attention(hidden, encoder_outputs)\n",
        "                \n",
        "        #a = [batch size, src len]\n",
        "        \n",
        "        a = a.unsqueeze(1)\n",
        "\n",
        "         #a = [batch size, 1, src len]\n",
        "        \n",
        "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
        "        \n",
        "        #encoder_outputs = [batch size, src sent len, enc hid dim * 2]\n",
        "        \n",
        "        weighted = torch.bmm(a, encoder_outputs)\n",
        "        \n",
        "        #weighted = [batch size, 1, enc hid dim * 2]\n",
        "        \n",
        "        weighted = weighted.permute(1, 0, 2)\n",
        "        \n",
        "        #weighted = [1, batch size, enc hid dim * 2]\n",
        "        \n",
        "        rnn_input = torch.cat((embedded, weighted), dim = 2)\n",
        "        \n",
        "        #rnn_input = [1, batch size, (enc hid dim * 2) + emb dim]\n",
        "            \n",
        "        output, hidden = self.rnn(rnn_input, hidden.unsqueeze(0))\n",
        "        \n",
        "        #output = [sent len, batch size, dec hid dim * n directions]\n",
        "        #hidden = [n layers * n directions, batch size, dec hid dim]\n",
        "        \n",
        "        #sent len, n layers and n directions will always be 1 in this decoder, therefore:\n",
        "        #output = [1, batch size, dec hid dim]\n",
        "        #hidden = [1, batch size, dec hid dim]\n",
        "        #this also means that output == hidden\n",
        "        assert (output == hidden).all()\n",
        "        \n",
        "        embedded = embedded.squeeze(0)\n",
        "        output = output.squeeze(0)\n",
        "        weighted = weighted.squeeze(0)\n",
        "        \n",
        "        output = self.out(torch.cat((output, weighted, embedded), dim = 1))\n",
        "        \n",
        "        #output = [bsz, output dim]\n",
        "        # print(\"Output.size() in Decoder forward: \", output.size())\n",
        "        return output, hidden.squeeze(0)\n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder, n_classes, device):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.DCNet = nn.Sequential(nn.Linear(encoder.enc_hid_dim*2,n_classes))\n",
        "        self.device = device\n",
        "        \n",
        "    def forward(self, src, trg, teacher_forcing_ratio = 0.5):\n",
        "        \n",
        "        #src = [src sent len, batch size]\n",
        "        #trg = [trg sent len, batch size]\n",
        "        #teacher_forcing_ratio is probability to use teacher forcing\n",
        "        #e.g. if teacher_forcing_ratio is 0.75 we use teacher forcing 75% of the time\n",
        "        \n",
        "        batch_size = src.shape[1]\n",
        "        max_len = trg.shape[0]\n",
        "        # print(\"max_len: \", max_len)\n",
        "        trg_vocab_size = self.decoder.output_dim\n",
        "        \n",
        "        #tensor to store decoder outputs\n",
        "        outputs = torch.zeros(max_len, batch_size, trg_vocab_size).to(self.device)\n",
        "        \n",
        "        #encoder_outputs is all hidden states of the input sequence, back and forwards\n",
        "        #hidden is the final forward and backward hidden states, passed through a linear layer\n",
        "        encoder_outputs, hidden = self.encoder(src)\n",
        "        predictions = self.DCNet(encoder_outputs[-1,:,:])\n",
        "                \n",
        "        #first input to the decoder is the <sos> tokens\n",
        "        input = trg[0,:]\n",
        "        \n",
        "        # print(\"Input.size in Seq2Seq forward: \", input.size())\n",
        "        for t in range(1, max_len):\n",
        "            #insert input token embedding, previous hidden state and all encoder hidden states\n",
        "            #receive output tensor (predictions) and new hidden state\n",
        "            output, hidden = self.decoder(input, hidden, encoder_outputs)\n",
        "            \n",
        "            #place predictions in a tensor holding predictions for each token\n",
        "            outputs[t] = output\n",
        "            \n",
        "            #decide if we are going to use teacher forcing or not\n",
        "            teacher_force = random.random() < teacher_forcing_ratio\n",
        "            # print(\"Teacher forcing on? \", teacher_force)\n",
        "            #get the highest predicted token from our predictions\n",
        "            # top1 = output.argmax(1) \n",
        "            \n",
        "            #if teacher forcing, use actual next token as next input\n",
        "            #if not, use predicted token\n",
        "            input = trg[t] if teacher_force else output\n",
        "            # print(\"Input shape used for next pass to decoder: \", input.shape)\n",
        "\n",
        "        return outputs,predictions\n",
        "\n",
        "INPUT_DIM = 20\n",
        "OUTPUT_DIM = 20\n",
        "ENC_HID_DIM = 32\n",
        "DEC_HID_DIM = 32\n",
        "ENC_N_LAYERS = 4\n",
        "DEC_N_LAYERS = 1\n",
        "ENC_DROPOUT = 0.2\n",
        "DEC_DROPOUT = 0.2\n",
        "N_CLASSES = 2\n",
        "datasetPaths = ['Preprocessed/Piano/train/','Preprocessed/Tabla/train/']\n",
        "assert N_CLASSES == len(datasetPaths)\n",
        "\n",
        "attn = Attention(ENC_HID_DIM, DEC_HID_DIM)\n",
        "enc = Encoder(INPUT_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_N_LAYERS, ENC_DROPOUT)\n",
        "decs = [Decoder(OUTPUT_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_N_LAYERS, DEC_DROPOUT, attn) for f in range(N_CLASSES)]\n",
        "\n",
        "models = [Seq2Seq(enc, dec, N_CLASSES, device).to(device) for dec in decs]\n",
        "\n",
        "def init_weights(m):\n",
        "    for name, param in m.named_parameters():\n",
        "        if 'weight' in name:\n",
        "            nn.init.normal_(param.data, mean=0, std=0.01)\n",
        "        else:\n",
        "            nn.init.constant_(param.data, 0)\n",
        "optimizers = [optim.Adam(model.parameters()) for model in models]\n",
        "pretrained = True\n",
        "pretrained_path = '.'\n",
        "if(pretrained):\n",
        "  for i,model in enumerate(models):\n",
        "    checkpoint = torch.load(pretrained_path + '/model%d.pth'%(i))\n",
        "    print(checkpoint.keys())\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "else:\n",
        "  for model in models:            \n",
        "    model.apply(init_weights)\n",
        "\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'Each model has {count_parameters(model):,} trainable parameters')\n",
        "\n",
        "criterion1 = nn.MSELoss()\n",
        "criterion2 = nn.CrossEntropyLoss()\n",
        "\n",
        "testsets = [InstrumentDataset('Preprocessed/Piano/test/'), InstrumentDataset('Preprocessed/Tabla/test/')]\n",
        "testloaders = [DataLoader(x, batch_size=75, shuffle=False, num_workers=8) for x in testsets]\n",
        "\n",
        "model1 = models[0]\n",
        "model2 = models[1]\n",
        "\n",
        "model1.eval()\n",
        "model2.eval()\n",
        "# os.makedirs('results/piano', exist_ok=True)\n",
        "with torch.no_grad():\n",
        "  for i,x in enumerate(testloaders[1]):\n",
        "    x = x.transpose(0,1)\n",
        "    x = x.to(device)\n",
        "    output = model1(x, x, 0)[0].transpose(0,1)\n",
        "    x = x.transpose(0,1).detach().cpu().numpy()\n",
        "    # x and output are batch_size x seq_len x filterbanks\n",
        "    print(output.shape, sig.shape, mu.shape, x.shape)\n",
        "    sig = sig.detach().cpu().numpy()\n",
        "    mu = mu.detach().cpu().numpy()\n",
        "    output = output.detach().cpu().numpy()\n",
        "    x = x\n",
        "    for j in range(output.shape[0]):\n",
        "      y_hat = librosa.feature.inverse.mfcc_to_audio(output[j,:,:].T)\n",
        "      sf.write('/content/drive/My Drive/results/tabla/tabla2piano_'+str(i*output.shape[1]+j)+'.wav', y_hat, 32000)\n",
        "      print('Saved: /content/drive/My Drive/results/tabla/tabla2piano_'+str(i*output.shape[1]+j)+'.wav')\n",
        "      y_hat = librosa.feature.inverse.mfcc_to_audio(x[j,:,:].T)\n",
        "      sf.write('/content/drive/My Drive/results/tabla/tabla_'+str(i*output.shape[1]+j)+'.wav', y_hat, 32000)\n",
        "      print('Saved: /content/drive/My Drive/results/tabla/tabla_'+str(i*output.shape[1]+j)+'.wav')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a9zNI5nZ-H9W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gahcjw7-AJde",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}